孟德斯鸠
拉德布鲁赫

CAP 理论: consistency(一致性), availability(可用性), partition tolerance(分区容错性)
一致性(强一致): 在分布式系统中的所有数据备份, 在同一时刻是否具有同样的值. 如果数据没有同步成功, 则返回失败, 不可返回旧的数据(如主从数据库的情况).
    分布式系统中, 经常用到主从(读写)分离(主数据库写, 从数据库读),
     此时如何实现一致性?
       1. 写入主数据库后要将数据同步到从数据库
       2. 写入主数据库后, 在向从数据库同步期间要将从数据库锁定, 待同步完成后释放锁, 此期间无法向外提供查询服务(即牺牲可用性)
          以免在新数据写入主数据库成功但未同步到从数据库之前, 向从数据库查询到旧的数据.
       对于 强一致性而言, 要么返回最新数据, 要么返回数据正在同步,查询失败, 不可以返回旧的数据.
       分布式系统一致性的特点:
       1. 由于存在数据同步的过程, 写操作的响应会有一定的延迟
       2. 为了保证数一致性会对资源暂时锁定, 待数据同步完成释放锁定资源
       3. 如果请求数据同步失败的节点则会返回错误信息, 一定不会返回旧数据
可用性: 在集群中一部分节点故障后, 集群整体是否还能响应的客户端的读写请求(可以接受旧的数据, 但是不可返回失败状态).
       任何事务操作都可以得到响应结果, 不接受响应超时和响应错误的情况.
     目标:
       1. 从数据库接收到数据查询请求则立即能响应数据查询结果
       2. 从数据库不允许出现响应超时或响应错误
     如何实现可用性?
       1. 写入主数据库后要将数据同步到从数据库
       2. 由于要保证从数据库的可用性, 不可将从数据库中的资源进行锁定
       3. 即使数据还没有同步完成, 从数据库也要返回要查询的数据, 哪怕是旧数据, 如果连旧数据也没有则可以
          按照约定返回一个默认信息, 但不能返回错误或响应超时
     分布式系统可用性的特点:
       1. 所有请求都有响应, 且不会出现响应超时或相应错误
分区容忍性: 以实际效果而言, 分区相当于对通信的时限要求. 系统如果不能在时限内达成数据一致性,
       就意味着发生了分区的情况, 必须就当前操作在 C 和 A 之间做出选择.
      通常分布式系统各个节点部署在不同的子网, 即网络分区, 不可避免会出现由于网络问题而导致节点之间通信失败,
      此时仍可对外提供服务, 这叫分区容忍性.
      目标:
        1. 主数据库向从数据库同步数据失败不影响读写操作
        2. 其中任意一个节点挂掉不影响另一个节点堆外提供服务
      如何实现分区容错性?
        1. 尽量使用异步取代同步操作, 例如使用异步方式将数据从数据库同步到从数据库,
           这样节点之间能有效的实现松耦合
        2. 添加从数据库节点, 其中一个从节点挂掉时其他从节点可提供服务(一主多从)
      分区容错性的特点:
        1. 分区容错性是分布式系统具备的基本能力
---
在所有分布式事务场景中不会同时具备 CAP 三个特性, 在满足 P 的情况下, C/A 不可能共存.
如果要实现 C 则必须保证数据一致性, 在数据同步的时候为防止向从数据库查询不一致的数据则需要将从数据库数据锁定,
待同步完成后解锁, 如果同步失败从数据库要返回错误信息或超时信息.
如果要实现 A 则必须保证数据可用性, 无论何时都可以向从数据库查询数据, 不会返回响应超时或错误信息

CAP原则的精髓就是要么AP,要么CP,要么AC,但是不存在CAP。
如果在某个分布式系统中数据无副本, 那么系统必然满足强一致性条件,
因为只有独一数据,不会出现数据不一致的情况,此时C和P两要素具备,
但是如果系统发生了网络分区状况或者宕机,必然导致某些数据不可以访问,
此时可用性条件就不能被满足,即在此情况下获得了CP系统.
由于网络硬件肯定会出现延迟丢包等问题, 所以 分区容错性 是必须要实现的.
所以只能在 一致性 和 可用性 之间进行权衡.
要么选择 CP: 要求 强一致性(consistency), 弱化可用性 (availability)
        zookeeper 即追求强一致性. 如跨行转账, 一次转账请求要求等待双方银行系统都完成整个事务才算完成.
要么选择 AP: 要求 高可用(availability), 放弃强一致性, 实现 最终一致性. (一般选择实现 AP)
注: 若选择 CA, 即放弃分区容错性, 也即不进行分区, 不考虑由于网络不通或节点挂掉的问题, 则可以实现一致性和可用性,
    那么系统将不是一个标准的分布式系统, 此时使用原始的关系型数据库的事务即可满足 CA.
    主从数据库之间不再进行数据同步, 数据库可以响应每次的查询请求, 通过事务隔离级别实现每个查询请求都可以返回最新的数据.
--
Base 理论:
    CAP 理论告诉我们一个分布式系统最多只能同时满足一致性 Consistency, 可用性 Availability, 分区容忍性 Partition tolerance
    三项中的两项. 其中 AP 在实际应用中较多, AP 即舍弃一致性, 保证可用性和分区容忍性, 但是在实际生产中很多场景都要实现一致性.
    比如主数据库向从数据库同步数据, 即使不要一致性, 但是最终也要将数据同步成功来保证数据一致性, 这种一致性和 CAP 中的一致性不同,
    CAP 中的一致性要求在任何时间查询每个节点数据都必须一致, 它强调的是强一致性, 但是最终一致性是允许可以在一段时间内每个节点的数据不一致,
    但是经过一段时间每个节点的数据必须一致, 此时强调数据最终一致性.
Basically Avaliable, Soft state, Eventually consistent
基本可用, 软状态, 最终一致性.
Base 是对 CAP 中 AP 的扩展, 通过牺牲强一致性来获得可用性, 当出现故障允许部分不可用但要保证核心功能可用, 允许数据在一段时间内是不一致的,
但最终达到一致状态. 满足 BASE 理论的事务, 称为 "柔性事务".
  基本可用:
    分布式系统在出现故障是, 允许损失部分可用功能, 保证核心功能可用. 如, 电商网站交易付款出现问题, 商品依然可以正常浏览
  软状态:
    由于不要求强一致性, 所以 BASE 允许系统存在中间状态(即 软状态), 这个状态不影响系统可用性,
    如订单的 "支付中", "数据同步中" 等状态, 待数据最终一致后状态改为 "成功" 状态.
  最终一致:
    最终一致是指经过一段时间后, 所有节点数据都将会达到一致. 如订单的 "支付中" 状态, 最终会变为 "支付成功" 或 "支付失败",
    使订单状态与实际校验结果达成一致, 但需要一定的延时.


AP 优于 CP
  分布式系统领域的 CAP 理论
  Consistency: 数据一致性, 即数据在存在多副本的情况下, 可能由于网络、机器故障、软件等问题导致数据写入部分副本成功,
          部分副本失败, 进而造成副本之间数据不一致, 存在冲突. 满足一致性则要求对数据的更新操作成功之后, 多副本的数据保持一致.
  Availability: 可用性. 在任何时候客户端对集群进行读写操作时, 请求能够正常响应, 即在一定的延时内完成.
  Partition Tolerance: 分区容错性. 即发生通信故障的时候, 整个集群被分隔为多个无法相互通信的分区时, 集群仍然可用.
  对于分布式系统来说, 一般网络条件相对不可控, 出现网络分区是不可避免的, 因此系统必须具备分区容忍性. 在这个前提下, 分布式系统
  的设计则在 AP 及 CP 之间进行选择. 不过不能理解为 CAP 三者之间必须三选二, 它们三者之间不是对等和可以相互替换的. 在分布式系统
  领域, P 是一个客观存在的事实, 不可绕过, 所以 P 与 AC 之间不是对等关系.


-------------------------2PC/3PC----------------------------------
2PC: 两阶段提交协议. two-phase commit protocol (prepare-commit)
    经典的强一致、中心化的原子提交协议.
    这里所说的 中心化 是指协议中有两类节点: 一个中心化协调者节点(coordinator)(也有叫 事务管理器 Transaction Manager)
        和 N 个参与者节点(participant)(也称 资源管理者 Resource Manager).
    在分布式系统中, 每个节点虽然可以知晓自己的操作是成功或失败, 却无法知道其他节点的操作的成功或失败.
    当一个事务跨越多个节点时, 为了保持事务的 ACID 特性, 需要引入一个作为 协调者 的组件来统一掌控所有参与者
    的操作并最终指示这些节点是否要把操作结果进行真正的提交.
    因此, 二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。
    所谓的两个阶段是指:
        第一个阶段: 准备阶段(投票阶段) prepare phase
            事务协调者给每个参与者发送Prepare消息，每个参与者要么直接返回失败，要么在本地执行事务，但不提交。
            可以进一步将准备阶段分为以下三个步骤：
            1）协调者节点向所有参与者节点询问是否可以执行提交操作，并开始等待各参与者节点的响应。
            2）参与者节点执行询问发起为止的所有事务操作，并将 Undo 信息和 Redo 信息写入日志。
            3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个"同意"消息；
               如果参与者节点的事务操作实际执行失败，则它返回一个"中止"消息。
            Undo-log 记录修改前的数据, 用于数据库 rollback, Redo-log 记录修改后的数, 用于 commit 事务后写入数据文件.
        第二个阶段: 提交阶段(执行阶段) commit/rollback
            如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚消息；
            否则，发送提交消息；参与者根据协调者的指令执行提交或者回滚操作。
            接下来分两种情况分别讨论提交阶段的过程。
            当协调者节点从所有参与者节点获得的相应消息都为"同意"时:
            1）协调者节点向所有参与者节点发出"正式提交"的请求。
            2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源。
            3）参与者节点向协调者节点发送"完成"消息。
            4）协调者节点受到所有参与者节点反馈的"完成"消息后，完成事务。
            如果任一参与者节点在第一阶段返回的响应消息为"中止"，
            或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：
            1）协调者节点向所有参与者节点发出"回滚操作"的请求。
            2）参与者节点利用之前写入的 Undo 信息执行回滚，并释放在整个事务期间内占用的资源。
            3）参与者节点向协调者节点发送"回滚完成"消息。
            4）协调者节点受到所有参与者节点反馈的"回滚完成"消息后，取消事务。
     2PC 问题:
        1、同步阻塞问题。
            执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
        2、单点故障。
            由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。
            尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。
            （如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）
        3、数据不一致。
            在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，
            这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。
            但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。
        4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。
            那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。
2PC 的实现(解决方案):
  1. XA 方案:
       2PC 的传统方案是在数据库层面实现的, 如 Oracle/MySQL 都支持 2PC 协议, 为了统一标准减少行业不必要的对接成本,
       需要制定标准化的处理模型及接口标准, 国际开放标准组织 Open Group 定义了分布式事务处理模型 (DTP Distributed Transaction Processing Reference Model)
       DTP 模型定义的角色:
         AP: Application Program, 可以理解为使用 DTP 分布式事务的程序.
         RM: Resource Manager, 资源管理器, 可以理解为事务的参与者, 一般情况下指一个数据库实例, 通过资源管理者对该数据进行控制, 资源管理者
             控制着分支事务
         TM: Transaction Manager, 事务管理者, 负责协调和管理事务, 事务管理器控制着全局事务, 管理事务声明周期, 并协调各个 RM.
             全局事务 是指分布式事务处理环境中, 需要操作多个数据库共同完成一个工作, 这个工作即是一个全局事务.
         DTP 模型定义 TM 和 RM 之间通讯的接口规范成为 XA, 简单理解为数据库提供的 2PC 接口协议, 基于数据库的 XA 协议来实现 2PC 又称为 XA 方案.
         以上三个角色之间的交互方式:
         1) TM 向 AP 提供应用程序接口, AP 通过 TM 提交及回滚事务;
         2) TM 交易中间件通过 XA 接口来通知 RM 数据库事务的开始, 结束, 提交, 回滚.
       整个 2PC 的事务流程涉及到三个角色 AP, RM, TM; AP 是指使用 2PC 分布式事务的应用程序, RM 指资源管理者, 控制着分支事务,
       TM 是事务管理器, 控制整个全局事务.
        1) 在 prepare 阶段, RM 执行实际的业务操作, 但不提交事务, 资源锁定
        2) 在 commit 阶段, TM 会接受 RM 在准备阶段的执行回复, 只要有任何一个 RM 执行失败, TM 会通知所有 RM 执行回滚操作,
           否则, TM 将会通知所有 RM 提交该事务, 提交阶段结束资源锁释放.
        XA 方案的问题:
            1. 需要本地数据库支持 XA 协议
            2. 资源所需要等到两个阶段结束才能释放, 性能较差
  2. Seata 方案
      传统 2PC 的问题在 Seata 中得到了解决, 它通过对本地关系数据库的分支事务的协调来驱动完成全局事务, 是工作在应用层的中间件.
      主要优点是性能较好, 且不长时间占用连接资源, 它以高效且对业务零入侵的方式解决微服务场景下面临的分布式事务问题, 它目前提供
      AT 模式(即 2PC) 和 TCC 模式的分布式事务解决方案.
    Seata 设计思路:
      Seata 的设计目标其一是对业务无入侵, 因此从业务五侵入的 2PC 方案着手, 在传统 2PC 的基础上演进, 并解决 2PC 方案面临的问题.
      Seata 把一个分布式事务理解成一个包含了若干分支事务的全局事务. 全局事务的职责是协调其下管辖的分支事务达成一致.
      要么一起成功提交, 要么一起失败回滚. 此外, 通常分支事务本身就是一个关系型数据库的本地事务.
      与传统 2PC 模型(XA 方案)类似, Seata 定义了 3 个组件来协调分布式事务的处理过程:
      TC: Transaction Coordinator, 事务协调者, 它是独立的中间件, 需要独立部署运行. 它维护全局事务的运行状态, 接受 TM 指令发起
         全局事务的提交和回滚, 负责与 RM 通信协调各个分支事务的提交或回滚.
      TM: Transaction Manager, 事务管理器, TM 需要嵌入应用程序中工作, 它负责开启一个全局事务, 并最终向 TC 发起全局提交或全局回滚的指令.
      RM: Resource Manager, 资源管理器, 控制分支事务, 负责分支注册, 状态汇报, 并接收 TC 的指令, 驱动分支(本地)事务的提交或回滚.
    Seata 实现 2PC 与传统 2PC 的差别:
      架构层次方面, 传统 2PC 方案的 RM 实际上是在数据库层, RM 本质上就是数据库自身, 通过 XA 协议实现, 而 Seata 的 RM 是以 jar 包形式作为中间件
      层部署在应用程序这一侧的.
      两阶段提交方面, 传统 2PC 无论第二阶段的决议是 commit 还是 rollback, 事务性资源的锁都要保持到 Phase2 完成才释放. 而 Seata
      的做法是在 Phase1 就将本地事务提交, 这样就可以省去 Phase2 持锁的时间, 整体提高效率.
    要点:
     1. 每个 RM 使用 DataSourceProxy 连接数据库, 其目的是使用 ConnectionProxy, 使用数据源和数据连接代理的目的就是在第一阶段将
        undo_log 和业务数据放在一个本地事务提交, 这样就保存了只要有业务操作就一定有 undo_log.
     2. 在第一阶段 undo_log 中存放数据修改前和修改后的值, 为事务回滚做好准备, 所以第一阶段完成就已经将分支事务提交, 也就释放了锁资源.
     3. TM 开启全局事务开始, 将 XID 全局事务 id 放在事务上下文中, 通过 feign 调用也将 XID 传入下游分支事务, 每个分支事务将自己的
        Branch ID 分支事务 ID 与 XID 关联.
     4. 第二阶段全局事务提交, TC 会通知各个分支参与者提交分支事务, 在第一阶段就已经提交了分支事务, 这里各个参与者只需要删除 undo_log
        即可, 并且可以异步执行, 第二阶段很快可以完成.
     5. 第二阶段全局事务回滚, TC 会通知各个分支参与者回滚分支事务, 通过 XID 和 Branch ID 找到相应的 undo_log,
        通过回滚日志生成反向的 SQL 并执行, 以完成分支事务回滚到初始状态, 如果回滚失败则会重试回滚操作.
分布式事务解决方法之 TCC
   何为 TCC 事务?
    Try, Confirm, Cancel.
    TCC 要求每个分支事务实现三个操作: 预处理 Try, 确认 Confirm, 撤销 Cancel. Try 操作做业务检查及资源预留, Confirm 做业务确认操作,
    Cancel 实现一个与 Try 相反的操作即回滚操作. TM 首先发起所有的分支事务的 try 操作, 任何一个分支事务的 try 操作执行失败, TM 将会发起
    所有分支事务的 Cancel 操作, 若 try 操作全部成功, TM 将会发起所有分支事务的 Confirm 操作, 其中 Confirm/Cancel 操作若执行失败,
    TM 会进行重试.
    1. Try 阶段是做业务检查(一致性)及资源预留(隔离), 此阶段仅是一个初步操作, 它和后续的 Confirm 一起才能真正构成一个完整的业务逻辑
    2. Confirm 阶段是做确认提交, Try 阶段所有分支事务执行成功后开始执行 Confirm. 通常情况下, 采用 TCC 则认为 Confirm 阶段是
       不会出错的. 即: 只要 Try 成功, Confirm 一定成功. 若 Confirm 阶段真的出错了, 需引入重试机制或人工处理.
    3. Cancel 阶段是在业务执行错误需要回滚的状态下执行分支事务的业务取消, 预留资源释放. 通常情况下, 采用 TCC 则认为 Cancel 阶段也是一定
       成功的. 若 Cancel 阶段真的出错了, 需引入重试机制或人工处理.
    4. TM 事务管理器
       TM 可以实现独立的服务, 也可以让全局事务发起方 充当 TM 的角色, TM 独立出来是为了成为公用的组件, 是为了考虑系统结构和软件复用.
    TM 在发起全局事务时生成全局事务记录 全局事务 ID 贯穿整个分布式事务调用链条, 用来记录事务上下文, 追踪和记录状态, 由于 Confirm 和 Cancel
    失败需进行重试, 因此需要实现为幂等, 幂等性是指同一个操作无论请求多少次, 其结果都相同.

try: 预留资源
confirm: 实际的执行与提交
cancel: 释放预留资源,
执行方式: try 全部成功, confirm 执行, 否则 cancel 执行; confirm/cancel 失败, 则重试
    TCC 认为, 通常 try 成功, 则 confirm/cancel 不会出错(而实际上若真的 confirm/cancel 出错, 则重试或人工干预处理)

TCC 三个注意问题
或者使用一张表, 标记一个 xid 此时的执行状态: init/confirmed/rollbacked
1. 空回滚 -- cancel
    没有执行 try 而直接 cancel, 应直接在 cancel 中返回.
    出现原因:
        当一个分支事务所在服务宕机或网络异常, 分支事务调用记录为失败, 这个时候其实没有执行 try 阶段,
        当故障恢复后, 分布式事务进行回滚则会调用二阶段的 cancel 方法, 从而形成空回滚.
    解决思路:
        关键是要识别出这个空回滚. 就是要知道一阶段是否执行, 如果执行了, 那就是正常回滚; 如果没执行, 那就是空回滚.
        可以用表分别记录记录 TCC 三个步骤(try/confirm/cancel 三张表)的执行记录, 或者使用 redis 锁等方式.
        有 try 记录就表示已执行 try, 正常回滚, 否则不存在, 空回滚.
2. 幂等 try/cancel 已经执行, 无需再次执行
   为了保证 TCC 二阶段提交重试机制不会引发数据不一致, 要求 TCC 的二阶段 Try/confirm/cancel 接口保证幂等,
   这样不会重复使用或者释放资源. 如果幂等没有控制好, 很可能导致数据不一致.
   解决思路:
     增加 confirm 记录, 每次 confirm 前都检查状态, 查证是否已经 confirm.
3. 悬挂 try: confirm/cancel 已经执行则无需执行 try. try 的资源永远无法释放--即悬挂.
   对于一个分布式事务, 其二阶段 cancel 接口比 try 接口先执行.
   本质原因: 一阶段与二阶段的执行顺序没有被严格保证
   出现原因:
    在 RPC 调用分布式事务 try 时, 先注册分支事务, 再执行 RPC 调用, 如果此时 RPC 调用的网络发生拥堵,
    通常 RPC 调用是有超时时间的, RPC 超时后, TM 就会通知 RM 回滚该分布式事务, 可能回滚完成后,
    RPC 请求才达到参与者真正执行, 而一个 try 方法预留的业务资源, 只有该分布式事务才能使用, 该分布式事务
    第一阶段预留的业务资源就再也没有人能处理了, 此即为悬挂, 即业务资源预留后没法继续处理.
   处理思路:
    如果二阶段执行完成, 那一阶段就不可再继续执行. 在执行一阶段事务时判断在该全局事务下, "分支事务记录"
    是否存在二阶段事务记录, 如有则不再执行 try(cancel 已经由于超时原因在 try 之前执行过了).


-------------------------------------------------
3PC: 三阶段提交协议. three-phase commit protocol
    和二阶段提交对比，三阶段提交主要是在2PC的第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。
    CanCommit 事务询问
        1、事务询问：
            协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。
        2、响应反馈：
            参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回YES响应，并进入预备状态。否则反馈NO
    PreCommit 事务执行
        协调者根据CanCommit阶段参与者的反应情况来决定是否可以进行事务的PreCommit操作。
        假如协调者从所有的参与者获得的反馈都是YES响应，那么就会执行事务的预执行：
        1、发送预提交请求：
            协调者向参与者发送PreCommit请求，并进入Prepared阶段。
        2、事务预提交：
            参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。
        3、响应反馈：
            如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。
        假如有任何一个参与者向协调者发送了NO响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。
        1、发送中断请求：
            协调者向所有参与者发送abort请求。
        2、中断事务：
            参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。
    DoCommit 事务提交
        该阶段进行真正的事务提交，也可以分为以下两种情况。
        如果协调证收到所有参与者的事务执行后的ACK响应，则发生如下事情：
        1、发送提交请求：
            协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。
        2、事务提交：
            参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
        3、响应反馈：
            事务提交完之后，向协调者发送Ack响应。
        4、完成事务：
            协调者接收到所有参与者的ack响应之后，完成事务。
        如果协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。
        1、发送中断请求：
            协调者向所有参与者发送abort请求
        2、事务回滚：
            参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。
        3、反馈结果：
            参与者完成事务回滚之后，向协调者发送ACK消息
        4、中断事务：
            协调者接收到参与者反馈的ACK消息之后，执行事务的中断
        还有一种情况，如果参与者无法及时接收到来自协调者的doCommit或者abort请求时，会在等待超时之后，会继续进行事务的提交。

 3PC比2PC好在哪？
 1、降低同步阻塞。
    在3PC中，第一阶段并没有让参与者直接执行事务，而是在第二阶段才会让参与者进行事务的执行。
    大大降低了阻塞的概率和时长。并且，在3PC中，如果参与者未收到协调者的消息，那么他会在等待一段时间后自动执行事务的commit，而不是一直阻塞。
 2、提升了数据一致性
     2PC中有一种情况会导致数据不一致，如在2PC的阶段二中，当协调者向参与者发送commit请求之后，发生了网络异常，
     只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。
     但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。
     这种情况在3PC的场景中得到了很好的解决，因为在3PC中，如果参与者没有收到协调者的消息时，他不会一直阻塞，
     过一段时间之后，他会自动执行事务。这就解决了那种协调者发出commit之后。
     另外，2PC还有个问题无法解决。那就是协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。
     那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。
     这种情况在3PC中是有办法解决的，因为在3PC中，选出新的协调者之后，他可以咨询所有参与者的状态，
     如果有某一个处于commit状态或者prepare-commit状态，那么他就可以通知所有参与者执行commit，否则就通知大家rollback。
     因为3PC的第三阶段一旦有机器执行了commit，那必然第一阶段大家都是同意commit的，所以可以放心执行commit。
 3PC 问题
 在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者abort请求时，会在等待超时之后，会继续进行事务的提交。
 所以，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。
 这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。
 所以，我们可以认为，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。



